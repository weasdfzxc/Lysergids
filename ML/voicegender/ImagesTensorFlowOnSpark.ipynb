{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import sys\n",
    "import cv2\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import tensorflow.python.platform\n",
    "\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import itertools\n",
    "\n",
    "class ConvNet(object): pass\n",
    "\n",
    "def init_weights(shape, st_dev):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev = st_dev))\n",
    "  \n",
    "def init_bias(shape, st_dev):\n",
    "    bias = tf.Variable(tf.random_normal(shape, stddev=st_dev))\n",
    "    return (bias)\n",
    "\n",
    "def model(X, w_layer_1, w_layer_2, w_layer_3,bias_1,bias_2,bias_3, p_keep_input = 1.0, p_keep_hidden = 1.0):\n",
    "    X = tf.nn.dropout(X, p_keep_input)\n",
    "    hidden_1 = tf.nn.relu(tf.add(tf.matmul(X, w_layer_1),bias_1))\n",
    "\n",
    "    hidden_1 = tf.nn.dropout(hidden_1, p_keep_hidden)\n",
    "    hidden_2 = tf.nn.relu(tf.add(tf.matmul(hidden_1, w_layer_2),bias_2))\n",
    "\n",
    "    hidden_2 = tf.nn.dropout(hidden_2, p_keep_hidden)\n",
    "\n",
    "    return tf.add(tf.matmul(hidden_2, w_layer_3),bias_3)\n",
    "\n",
    "def convert_label(result):\n",
    "    resultx = []\n",
    "    for res in result:\n",
    "        restmp = []\n",
    "        if res == 'male':\n",
    "            restmp = [0,1]\n",
    "        elif res == 'female':\n",
    "            restmp = [1,0]\n",
    "        resultx.append(restmp)\n",
    "    return resultx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 30\n",
    "def resize_image(image, height = IMAGE_SIZE, width = IMAGE_SIZE):\n",
    "    top, bottom, left, right = (0, 0, 0, 0)\n",
    "    \n",
    "    #获取图像尺寸\n",
    "    h, w, _ = image.shape\n",
    "    \n",
    "    #对于长宽不相等的图片，找到最长的一边\n",
    "    longest_edge = max(h, w)    \n",
    "    \n",
    "    #计算短边需要增加多上像素宽度使其与长边等长\n",
    "    if h < longest_edge:\n",
    "        dh = longest_edge - h\n",
    "        top = dh // 2\n",
    "        bottom = dh - top\n",
    "    elif w < longest_edge:\n",
    "        dw = longest_edge - w\n",
    "        left = dw // 2\n",
    "        right = dw - left\n",
    "    else:\n",
    "        pass \n",
    "    \n",
    "    #RGB颜色\n",
    "    BLACK = [0, 0, 0]\n",
    "    \n",
    "    #给图像增加边界，是图片长、宽等长，cv2.BORDER_CONSTANT指定边界颜色由value指定\n",
    "    constant = cv2.copyMakeBorder(image, top , bottom, left, right, cv2.BORDER_CONSTANT, value = BLACK)\n",
    "    \n",
    "    #调整图像大小并返回\n",
    "    return cv2.resize(constant, (height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_path(path_name):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for dir_item in os.listdir(path_name):\n",
    "        #从初始路径开始叠加，合并成可识别的操作路径\n",
    "        full_path = os.path.abspath(os.path.join(path_name, dir_item))\n",
    "        \n",
    "        if os.path.isdir(full_path):    #如果是文件夹，继续递归调用\n",
    "            read_path(full_path)\n",
    "        else:   #文件\n",
    "            if dir_item.endswith('.jpg'):\n",
    "                image = cv2.imread(full_path)\n",
    "                image = resize_image(image, IMAGE_SIZE, IMAGE_SIZE)\n",
    "                \n",
    "                #图片变成灰度图\n",
    "                #cv2.imwrite('1.jpg', image)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY );\n",
    "                #xd,image = cv2.threshold(image,127,255,cv2.THRESH_BINARY)\n",
    "                imagex = []\n",
    "                for img in image:\n",
    "                    imagex.extend(img)\n",
    "                images.append(imagex)                \n",
    "                labels.append(path_name)                                \n",
    "                    \n",
    "    return images,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path_name):\n",
    "    images,labels = read_path(path_name)    \n",
    "  \n",
    "    labels = [[0,1] if label.endswith('camera') else [1,0] for label in labels]\n",
    "   \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_dataset(\"/dbfs/FileStore/tables/images/category/camera\")\n",
    "imagesx, labelsx = load_dataset(\"/dbfs/FileStore/tables/images/category/lotus\")\n",
    "images.extend(imagesx)\n",
    "labels.extend(labelsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, valid_images, train_labels, valid_labels = train_test_split(images, labels, test_size = 0.1, random_state = random.randint(0, 100)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 30\n",
    "num_epochs = 250\n",
    "train_size = len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_func(base_learning_rate = 0.0005,decay_rate = 0.9,conv1_size=1200, fc1_size=400):\n",
    "    \n",
    "#     training_epochs = 50\n",
    "#     learning_rate = 0.0005\n",
    "#     decay_rate = 0.9\n",
    "#     momentum=0.001\n",
    "    \n",
    "    X = tf.placeholder(\"float\", [None, 900])\n",
    "    Y = tf.placeholder(\"float\", [None, 2])\n",
    "\n",
    "\n",
    "    w_layer_1 = init_weights([900, conv1_size], st_dev=0.01)\n",
    "    bias_1 = init_bias(shape=[conv1_size], st_dev=0.01)\n",
    "\n",
    "    w_layer_2 = init_weights([conv1_size, fc1_size], st_dev=0.01)\n",
    "    bias_2 = init_bias(shape=[fc1_size], st_dev=0.01)\n",
    "\n",
    "    w_layer_3 = init_weights([fc1_size, 2], st_dev=0.01)\n",
    "    bias_3 = init_bias(shape=[2], st_dev=0.01)\n",
    "\n",
    "    p_keep_input = tf.placeholder(\"float\")\n",
    "\n",
    "    p_keep_hidden = tf.placeholder(\"float\")\n",
    "\n",
    "    # 构建模型\n",
    "    py_x = model(X, w_layer_1, w_layer_2, w_layer_3,bias_1,bias_2,bias_3, p_keep_input, p_keep_hidden)\n",
    "    \n",
    "    batch = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "      base_learning_rate,                # Base learning rate.\n",
    "      batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "      train_size,          # Decay step.\n",
    "      decay_rate,                # Decay rate.\n",
    "      staircase=True)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=py_x, labels=Y))\n",
    "    train_op = tf.train.RMSPropOptimizer(learning_rate, decay_rate, 0.001).minimize(cost)\n",
    "    predict_op = tf.argmax(py_x, 1)\n",
    "    \n",
    "    res = ConvNet()\n",
    "    res.prediction = tf.argmax(py_x, 1)\n",
    "    res.optimizer = train_op\n",
    "    res.loss = cost\n",
    "    res.p_keep_input = p_keep_input\n",
    "    res.p_keep_hidden = p_keep_hidden\n",
    "    res.decay_rate = decay_rate\n",
    "    res.learning_rate = learning_rate\n",
    "    res.train_data_node = X\n",
    "    res.train_labels_node = Y\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rates = [float(x) for x in np.logspace(-5, -3, num=10, base=10.0)]\n",
    "decay_rates = [0.9,0.95]\n",
    "#conv1_sizes = [16,32,64]\n",
    "fc1_sizes = [100,200,400,800,1000]\n",
    "all_experiments = list(itertools.product(base_learning_rates, decay_rates, fc1_sizes))\n",
    "print(len(all_experiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_bc = sc.broadcast(train_images)\n",
    "train_labels_bc = sc.broadcast(train_labels)\n",
    "\n",
    "def run(base_learning_rate, decay_rate, fc1_size):\n",
    "    train_data = train_data_bc.value\n",
    "    train_labels = train_labels_bc.value\n",
    "    res = {}\n",
    "    res['base_learning_rate'] = base_learning_rate\n",
    "    res['decay_rate'] = decay_rate\n",
    "    res['fc1_size'] = fc1_size\n",
    "    res['accuracy'] = 0.0\n",
    "    res['validation_accuracy'] = 0.0\n",
    "    \n",
    "    with tf.Session() as s:\n",
    "        # Create the computation graph\n",
    "        graph = image_func(base_learning_rate, decay_rate, fc1_size=fc1_size)\n",
    "        # Run all the initializers to prepare the trainable parameters.\n",
    "        tf.global_variables_initializer().run()\n",
    "        # Loop through training steps.\n",
    "#         for step in xrange(num_epochs * train_size // BATCH_SIZE):\n",
    "#             # Compute the offset of the current minibatch in the data.\n",
    "#             # Note that we could use better randomization across epochs.\n",
    "#             offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "#             batch_data = train_data[offset:(offset + BATCH_SIZE)]\n",
    "#             batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "#             feed_dict = {graph.train_data_node: batch_data,\n",
    "#                            graph.train_labels_node: batch_labels}\n",
    "#             s.run(graph.optimizer,feed_dict=feed_dict)\n",
    "        for i in range(num_epochs):\n",
    "            for start, end in zip(range(0, len(train_labels), BATCH_SIZE), range(BATCH_SIZE, len(train_labels), BATCH_SIZE)):\n",
    "                s.run(graph.optimizer, feed_dict = {graph.train_data_node: train_images[start:end], graph.train_labels_node: train_labels[start:end], graph.p_keep_input: 0.8, graph.p_keep_hidden: 0.5})\n",
    "        res['accuracy'] = np.mean(np.argmax(valid_labels, axis = 1) == s.run(graph.prediction, \n",
    "                          feed_dict = {graph.train_data_node: valid_images, graph.train_labels_node: valid_labels, graph.p_keep_input: 1.0, graph.p_keep_hidden: 1.0}))\n",
    "        res['validation_accuracy'] = np.mean(np.argmax(train_labels, axis = 1) == s.run(graph.prediction, \n",
    "                          feed_dict = {graph.train_data_node: train_images, graph.train_labels_node: train_labels, graph.p_keep_input: 1.0, graph.p_keep_hidden: 1.0}))\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(0.0005,0.9,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 4\n",
    "n = max(2, int(len(all_experiments) // num_nodes))\n",
    "grouped_experiments = [all_experiments[i:i+n] for i in range(0, len(all_experiments), n)]\n",
    "all_exps_rdd = sc.parallelize(grouped_experiments, numSlices=len(grouped_experiments))\n",
    "results = all_exps_rdd.flatMap(lambda z: [run(*y) for y in z]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "for fc1_size in fc1_sizes:\n",
    "    #ax = df[(df.decay_rate == 0.90) & (df.fc1_size == fc1_size)].plot(x='base_learning_rate', y='accuracy')\n",
    "    ax.plot(df[(df.decay_rate == 0.95) & (df.fc1_size == fc1_size)][\"base_learning_rate\"],df[(df.decay_rate == 0.90) & (df.fc1_size == fc1_size)][\"accuracy\"])\n",
    "lines = ax.get_lines()\n",
    "ax.legend(lines, [str(x) for x in fc1_sizes], loc='upper right')\n",
    "ax.set_ylabel(\"Test Accuracy\")\n",
    "ax.set_xlabel(\"Base learning rate\")\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "name": "VoiceFunc",
  "notebookId": 99304367822830
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
